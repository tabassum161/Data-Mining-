# -*- coding: utf-8 -*-
"""dm project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TdU6Mnce0BJz4fiwP3NdVuCBinw5wdU-
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import (
    Input, Embedding, LSTM, GRU, Conv1D, GlobalMaxPooling1D,
    Dense, Dropout, LayerNormalization, MultiHeadAttention
)
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

class FairCLIPLoader:
    def __init__(self, file_path=None):
        self.file_path = file_path
        self.data = None

    def load_data(self):
        try:
            self.data = pd.read_csv(self.file_path,
                                  encoding='utf-8',
                                  dtype={
                                      'age': float,
                                      'gender': str,
                                      'race': str,
                                      'ethnicity': str,
                                      'language': str,
                                      'maritalstatus': str,
                                      'note': str,
                                      'gpt4_summary': str,
                                      'glaucoma': str,
                                      'use': str
                                  })
            print(f"Successfully loaded {len(self.data)} records")
            return self.data
        except Exception as e:
            print(f"Error loading CSV file: {str(e)}")
            return None

    def preprocess_data(self):
        if self.data is None:
            self.load_data()

        if self.data is None:
            return None

        try:
            # Clean text data
            self.data['note'] = self.data['note'].fillna('')
            self.data['note'] = self.data['note'].astype(str).str.lower()
            self.data['note'] = self.data['note'].apply(self._clean_medical_text)

            # Convert glaucoma labels to binary
            self.data['label'] = (self.data['glaucoma'].str.lower() == 'yes').astype(int)

            # Standardize race categories
            self.data['race'] = self.data['race'].str.strip().str.capitalize()

            # Filter to keep only main racial categories
            valid_races = ['Asian', 'Black', 'White']
            self.data = self.data[self.data['race'].isin(valid_races)]

            # Create splits
            train_data, test_data = train_test_split(self.data, test_size=0.2, random_state=42, stratify=self.data[['race', 'label']])
            train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data[['race', 'label']])

            return {
                'train': self._prepare_split_data(train_data),
                'val': self._prepare_split_data(val_data),
                'test': self._prepare_split_data(test_data)
            }

        except Exception as e:
            print(f"Error preprocessing data: {str(e)}")
            return None

    def _clean_medical_text(self, text):
        text = str(text)
        text = ' '.join(text.split())
        text = text.replace('\n', ' ').replace('\r', ' ')
        text = ''.join(c for c in text if c.isalnum() or c in '., -')
        return text

    def _prepare_split_data(self, data):
        return {
            'notes': data['note'].values,
            'labels': data['label'].values,
            'race': data['race'].values
        }

class TextPreprocessor:
    def __init__(self, max_words=10000, max_len=500):
        self.tokenizer = Tokenizer(num_words=max_words)
        self.max_len = max_len

    def fit_transform(self, texts):
        self.tokenizer.fit_on_texts(texts)
        sequences = self.tokenizer.texts_to_sequences(texts)
        return pad_sequences(sequences, maxlen=self.max_len)

    def transform(self, texts):
        sequences = self.tokenizer.texts_to_sequences(texts)
        return pad_sequences(sequences, maxlen=self.max_len)

class ModelBuilder:
    def __init__(self, max_words=10000, max_len=500, embed_dim=100):
        self.max_words = max_words
        self.max_len = max_len
        self.embed_dim = embed_dim

    def build_lstm(self):
        inputs = Input(shape=(self.max_len,))
        x = Embedding(self.max_words, self.embed_dim)(inputs)
        x = LSTM(64, return_sequences=True)(x)
        x = LSTM(32)(x)
        x = Dense(16, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        model = Model(inputs, outputs)
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

    def build_gru(self):
        inputs = Input(shape=(self.max_len,))
        x = Embedding(self.max_words, self.embed_dim)(inputs)
        x = GRU(64, return_sequences=True)(x)
        x = GRU(32)(x)
        x = Dense(16, activation='relu')(x)
        outputs = Dense(1, activation='sigmoid')(x)
        model = Model(inputs, outputs)
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

    def build_transformer(self):
        inputs = Input(shape=(self.max_len,))
        x = Embedding(self.max_words, self.embed_dim)(inputs)

        attention_output = MultiHeadAttention(num_heads=8, key_dim=self.embed_dim)(x, x)
        x = LayerNormalization(epsilon=1e-6)(attention_output + x)
        x = Dense(self.embed_dim * 4, activation='relu')(x)
        x = Dense(self.embed_dim)(x)
        x = LayerNormalization(epsilon=1e-6)(x)

        x = GlobalMaxPooling1D()(x)
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.1)(x)
        outputs = Dense(1, activation='sigmoid')(x)

        model = Model(inputs, outputs)
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

class ModelEvaluator:
    def __init__(self):
        self.results = {}

    def evaluate(self, model, X_test, y_test, race_test):
        predictions = model.predict(X_test)

        # Overall metrics
        overall_auc = roc_auc_score(y_test, predictions)

        # Race-specific metrics
        race_metrics = {}
        for race in np.unique(race_test):
            mask = race_test == race
            if sum(mask) > 0:
                race_metrics[race] = {
                    'auc': roc_auc_score(y_test[mask], predictions[mask]),
                    'count': sum(mask)
                }

        return {
            'overall_auc': overall_auc,
            'race_metrics': race_metrics
        }

    def plot_results(self, save_path=None):
        plt.figure(figsize=(15, 5))

        # Plot overall AUC
        plt.subplot(1, 2, 1)
        models = list(self.results.keys())
        overall_aucs = [res['overall_auc'] for res in self.results.values()]

        plt.bar(models, overall_aucs)
        plt.title('Overall AUC by Model')
        plt.xticks(rotation=45)
        plt.ylim(0, 1)

        # Plot race-specific AUCs
        plt.subplot(1, 2, 2)
        for model_name, results in self.results.items():
            races = list(results['race_metrics'].keys())
            aucs = [results['race_metrics'][race]['auc'] for race in races]
            plt.plot(races, aucs, marker='o', label=model_name)

        plt.title('AUC by Race and Model')
        plt.xticks(rotation=45)
        plt.legend()
        plt.ylim(0, 1)

        plt.tight_layout()
        if save_path:
            plt.savefig(save_path)
        plt.show()

def train_and_evaluate(model_name, model, X_train, y_train, X_val, y_val, X_test, y_test, race_test, epochs=10, batch_size=32):
    print(f"\nTraining {model_name}...")

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        verbose=1
    )

    evaluator = ModelEvaluator()
    results = evaluator.evaluate(model, X_test, y_test, race_test)

    print(f"\n{model_name} Results:")
    print(f"Overall AUC: {results['overall_auc']:.3f}")
    print("\nAUC by Race:")
    for race, metrics in results['race_metrics'].items():
        print(f"{race}: {metrics['auc']:.3f} (n={metrics['count']})")

    return results



def main():
    # Initialize components
    loader = FairCLIPLoader('/content/sample_data/clinical_notes.csv')
    data = loader.preprocess_data()

    if data is None:
        return

    # Initialize text preprocessor
    preprocessor = TextPreprocessor()

    # Prepare data
    X_train = preprocessor.fit_transform(data['train']['notes'])
    X_val = preprocessor.transform(data['val']['notes'])
    X_test = preprocessor.transform(data['test']['notes'])

    y_train = data['train']['labels']
    y_val = data['val']['labels']
    y_test = data['test']['labels']

    race_test = data['test']['race']

    # Initialize model builder
    model_builder = ModelBuilder()

    # Initialize evaluator
    evaluator = ModelEvaluator()

    # Train and evaluate models
    models = {
        'LSTM': model_builder.build_lstm(),
        'GRU': model_builder.build_gru(),
        'Transformer': model_builder.build_transformer()
    }

    for model_name, model in models.items():
        results = train_and_evaluate(
            model_name, model,
            X_train, y_train,
            X_val, y_val,
            X_test, y_test,
            race_test
        )
        evaluator.results[model_name] = results

    # Plot results
    evaluator.plot_results('model_comparison.png')

if __name__ == "__main__":
    main()



